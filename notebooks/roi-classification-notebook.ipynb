{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Team Assignment 2 - Predicting ROI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *IMPORTANT*: This is an outline of what needs to be discussed. Please fill in where you see fit and feel free to add more. If you don't understand something, I can explain. Please not the other files and folders in this project as they probably will anwser a lot of your questions. Also, you may have to play around with the paths since I use abosulte paths for everything which is relative to my VM. Overall, try to add as much detail as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Overview\n",
    "Start with an introduction that outlines the project's objectives, the significance of predicting ROI, and a brief overview of the approach taken. This section sets the stage for what the notebook will cover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview\n",
    "Before diving into the code, provide an overview of the data used for this project. Mention:\n",
    "\n",
    "The source of the data.\n",
    "Key features/variables and their relevance to the project.\n",
    "Any preprocessing done outside of the notebook (e.g., data cleaning, feature selection) and the rationale behind these choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Computational Environment\n",
    "Explain that the computational work, especially model training, was performed using UNC's Longleaf services due to the high computational requirements. Offer insight into why local machines were not suitable and how Longleaf services benefited the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Import and Initial Setup\n",
    "Document how to set up the environment within the notebook, including importing necessary libraries and loading the dataset. If the data was preprocessed or features were selected outside the notebook, include code or pseudo-code snippets to show how this was done, or describe the process if code cannot be shared. Discuss why we used a virtual environment. Discuss project struture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Setting the Correct Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "# This should be the path to the directory containing 'notebooks' and 'utils'\n",
    "project_root = Path.cwd().parent  # If your notebook is directly inside the 'notebooks' directory\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that you are in the right directory\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Importing Needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.utility as utility\n",
    "import imputers.earliest_cr_line_d as ecld\n",
    "import imputers.numeric_data as nd\n",
    "# We first import a number of libraries that we will be using in today's class\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting packages we'll use\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Rather than importing the whole sklearn library, we will import only certain modules\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from joblib import load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Importing Accepted Loan Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_accepted = '../data/LendingClub/accepted_2007_to_2018Q4.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Read Accpeted Data into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accepted = pd.read_csv(path_to_accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rows and columns\n",
    "print(df_accepted.shape)\n",
    "df_accepted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Primary Data Inspection and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Define a List of Features Available for us at Loan's Origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_features = [\"acc_now_delinq\", \"acc_open_past_24mths\", \"addr_state\",\"all_util\",\n",
    "    \"annual_inc\",\"annual_inc_joint\",\"application_type\",\"avg_cur_bal\",\"bc_open_to_buy\",\n",
    "    \"bc_util\",\"chargeoff_within_12_mths\",\"collections_12_mths_ex_med\",\"delinq_2yrs\",\n",
    "    \"dti\",\"dti_joint\",\"earliest_cr_line\",\"emp_length\",\"fico_range_high\",\"fico_range_low\",\n",
    "    \"funded_amnt\",\"funded_amnt_inv\",\"grade\",\"home_ownership\",\"il_util\",\"initial_list_status\",\n",
    "    \"inq_fi\",\"inq_last_12m\",\"inq_last_6mths\",\"installment\",\"int_rate\",\"issue_d\",\n",
    "    \"loan_amnt\",\"max_bal_bc\",\"mort_acc\",\"mths_since_last_delinq\",\"mths_since_last_major_derog\",\n",
    "    \"mths_since_last_record\",\"mths_since_rcnt_il\",\"mths_since_recent_bc\",\"mths_since_recent_inq\",\n",
    "    \"num_accts_ever_120_pd\",\"num_actv_bc_tl\",\"num_actv_rev_tl\",\"num_bc_sats\",\"num_bc_tl\",\"num_il_tl\",\n",
    "    \"num_op_rev_tl\",\"num_rev_accts\",\"num_rev_tl_bal_gt_0\",\"num_sats\",\"num_tl_120dpd_2m\",\"num_tl_30dpd\",\n",
    "    \"num_tl_90g_dpd_24m\",\"num_tl_op_past_12m\",\"open_acc\",\"open_acc_6m\",\n",
    "    \"open_il_12m\", \"open_il_24m\",\"open_act_il\",\"open_rv_12m\",\"open_rv_24m\",\"pct_tl_nvr_dlq\",\n",
    "    \"percent_bc_gt_75\",\"pub_rec\",\"pub_rec_bankruptcies\",\"purpose\",\"revol_bal\",\"revol_util\",\n",
    "    \"sub_grade\",\"tax_liens\",\"term\",\"tot_coll_amt\",\"tot_cur_bal\",\"tot_hi_cred_lim\",\"total_acc\",\n",
    "    \"total_bal_ex_mort\",\"total_bal_il\",\"total_bc_limit\",\"total_cu_tl\",\"total_il_high_credit_limit\",\n",
    "    \"verification_status\",\"verified_status_joint\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Drop Useless Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the column names based on their positions\n",
    "columns_to_drop = ['id', 'member_id', 'url', 'zip_code']\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df_accepted = df_accepted.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Examine Features (Discuss Implications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accepted.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accepted.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Dropping Columns with a High Null Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop where the null count is greater than or equal to 50% of samples.\n",
    "df_accepted, null_columns = utility.drop_null_columns(df_accepted)\n",
    "classifier_features = [item for item in classifier_features if item not in null_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View null columns\n",
    "print(null_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View df's columns\n",
    "print(list(df_accepted.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Imputing Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accepted.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all columns that could be dates (object type)\n",
    "potential_date_columns = utility.get_object_columns(df_accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display each object with its value count\n",
    "value_counts = utility.display_value_counts(df_accepted, potential_date_columns)\n",
    "value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows where issue_d is null\n",
    "df_accepted = df_accepted.dropna(subset=['issue_d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type cast all date columns to date objects\n",
    "date_columns = ['issue_d', 'earliest_cr_line']\n",
    "df_accepted = utility.to_datetime(df=df_accepted, columns=date_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1 Create 'fico_descriptor' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               Good\n",
       "1               Good\n",
       "2               Good\n",
       "3          Very Good\n",
       "4               Good\n",
       "             ...    \n",
       "2260694         Fair\n",
       "2260695         Good\n",
       "2260696         Good\n",
       "2260697         Fair\n",
       "2260698         Fair\n",
       "Name: fico_descriptor, Length: 2260668, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_accepted = utility.get_fico_descriptor(df=df_accepted)\n",
    "df_accepted['fico_descriptor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing dates\n",
    "ecld_imputer = ecld.EarliestCRLineDateImputer()\n",
    "ecld_imputer.fit(X=df_accepted)\n",
    "df_accepted = ecld_imputer.transform(X=df_accepted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Imputing Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binned data\n",
    "df_accepted = df_accepted[(df_accepted.emp_length != '10+ years') &\n",
    "                          (df_accepted.emp_length != '< 1 year')]\n",
    "\n",
    "# Make the rest categorical \n",
    "category_columns = utility.get_object_columns(df=df_accepted)\n",
    "result_list = [item for item in category_columns if item not in date_columns]\n",
    "df_accepted = utility.to_categorical(df=df_accepted, columns=result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute categorical data using mode\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df_accepted[category_columns] = imputer.fit_transform(df_accepted[category_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accepted.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Imputing Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/aryonna/488-team-assignment-2/imputers/numeric_data.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_transformed[column].fillna(self.medians_[column], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "numerical_columns = utility.get_numerical_columns(df_accepted)\n",
    "columns_for_grouping_and_median = numerical_columns + ['fico_descriptor']\n",
    "imputer = nd.NumericDataImputer(group_column='fico_descriptor')\n",
    "imputer.fit(df_accepted[columns_for_grouping_and_median])\n",
    "df_accepted[columns_for_grouping_and_median] = imputer.transform(df_accepted[columns_for_grouping_and_median])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accepted.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fixing Skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Examine data's distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # None means unlimited\n",
    "df_accepted.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the columns with high skew values as these need to be fixed later\n",
    "skewed_columns = utility.get_high_skewed_columns(df=df_accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>last_fico_range_high</th>\n",
       "      <th>il_util</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.322675e+06</td>\n",
       "      <td>1.322675e+06</td>\n",
       "      <td>1.322675e+06</td>\n",
       "      <td>1.322675e+06</td>\n",
       "      <td>1.322675e+06</td>\n",
       "      <td>1.322675e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.440358e+04</td>\n",
       "      <td>1.439811e+04</td>\n",
       "      <td>1.437863e+04</td>\n",
       "      <td>1.314650e+01</td>\n",
       "      <td>6.859285e+02</td>\n",
       "      <td>7.156951e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.946151e+03</td>\n",
       "      <td>8.944043e+03</td>\n",
       "      <td>8.947832e+03</td>\n",
       "      <td>4.824584e+00</td>\n",
       "      <td>7.380668e+01</td>\n",
       "      <td>1.740894e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000e+02</td>\n",
       "      <td>5.000000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.310000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.675000e+03</td>\n",
       "      <td>7.675000e+03</td>\n",
       "      <td>7.575000e+03</td>\n",
       "      <td>9.670000e+00</td>\n",
       "      <td>6.540000e+02</td>\n",
       "      <td>6.800000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.200000e+04</td>\n",
       "      <td>1.200000e+04</td>\n",
       "      <td>1.200000e+04</td>\n",
       "      <td>1.269000e+01</td>\n",
       "      <td>6.990000e+02</td>\n",
       "      <td>7.300000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>1.599000e+01</td>\n",
       "      <td>7.340000e+02</td>\n",
       "      <td>7.700000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000e+04</td>\n",
       "      <td>4.000000e+04</td>\n",
       "      <td>4.000000e+04</td>\n",
       "      <td>3.099000e+01</td>\n",
       "      <td>8.500000e+02</td>\n",
       "      <td>5.580000e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          loan_amnt   funded_amnt  funded_amnt_inv      int_rate  \\\n",
       "count  1.322675e+06  1.322675e+06     1.322675e+06  1.322675e+06   \n",
       "mean   1.440358e+04  1.439811e+04     1.437863e+04  1.314650e+01   \n",
       "std    8.946151e+03  8.944043e+03     8.947832e+03  4.824584e+00   \n",
       "min    5.000000e+02  5.000000e+02     0.000000e+00  5.310000e+00   \n",
       "25%    7.675000e+03  7.675000e+03     7.575000e+03  9.670000e+00   \n",
       "50%    1.200000e+04  1.200000e+04     1.200000e+04  1.269000e+01   \n",
       "75%    2.000000e+04  2.000000e+04     2.000000e+04  1.599000e+01   \n",
       "max    4.000000e+04  4.000000e+04     4.000000e+04  3.099000e+01   \n",
       "\n",
       "       last_fico_range_high       il_util  \n",
       "count          1.322675e+06  1.322675e+06  \n",
       "mean           6.859285e+02  7.156951e+01  \n",
       "std            7.380668e+01  1.740894e+01  \n",
       "min            0.000000e+00  0.000000e+00  \n",
       "25%            6.540000e+02  6.800000e+01  \n",
       "50%            6.990000e+02  7.300000e+01  \n",
       "75%            7.340000e+02  7.700000e+01  \n",
       "max            8.500000e+02  5.580000e+02  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skewed_df = df_accepted[skewed_columns]\n",
    "skewed_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize skewed features\n",
    "utility.visualize_numerical_variables(skewed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  log_df[f + '_log']=np.log1p(log_df[f])\n",
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  log_df[f + '_log']=np.log1p(log_df[f])\n",
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  log_df[f + '_log']=np.log1p(log_df[f])\n",
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  log_df[f + '_log']=np.log1p(log_df[f])\n",
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  log_df[f + '_log']=np.log1p(log_df[f])\n",
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  log_df[f + '_log']=np.log1p(log_df[f])\n"
     ]
    }
   ],
   "source": [
    "# Fix skewnes with log\n",
    "log_df = utility.fix_skewed_features(log_df=df_accepted, skewed_features=skewed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Drop outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = utility.remove_outliers(df=log_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Standardize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "numeric_columns = utility.get_numerical_columns(df=log_df)\n",
    "log_df = utility.scale_numeric(log_df, numeric_columns, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a copy\n",
    "log_df_preFE = log_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3739205/2696554352.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  log_df['credit_history_length'] = (log_df['issue_d'] - log_df['earliest_cr_line']).dt.days / 365.25\n",
      "/tmp/ipykernel_3739205/2696554352.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  log_df['PTI_ratio'] = log_df['installment'] / (log_df['annual_inc']/12)\n",
      "/tmp/ipykernel_3739205/2696554352.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  log_df['mnthly_disposable_income'] = (log_df['annual_inc'] / 12) - log_df['installment']\n",
      "/tmp/ipykernel_3739205/2696554352.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  log_df['investor_confidence'] = log_df['funded_amnt_inv_log'] / log_df['loan_amnt_log']\n",
      "/tmp/ipykernel_3739205/2696554352.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  log_df['avg_utilization_ratio'] = (log_df['revol_util'] + log_df['bc_util']) / 2\n",
      "/tmp/ipykernel_3739205/2696554352.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  log_df['delinquency_score'] = log_df['acc_now_delinq'] + log_df['delinq_2yrs'] + log_df['num_accts_ever_120_pd']\n",
      "/tmp/ipykernel_3739205/2696554352.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  log_df['credit_inquiry_impact'] = (log_df['inq_last_12m'] * 0.5) + (log_df['inq_last_6mths'] * 0.75) + log_df['inq_fi']\n"
     ]
    }
   ],
   "source": [
    "# Credit history\n",
    "log_df['credit_history_length'] = (log_df['issue_d'] - log_df['earliest_cr_line']).dt.days / 365.25\n",
    "\n",
    "# PTI ratio\n",
    "log_df['PTI_ratio'] = log_df['installment'] / (log_df['annual_inc']/12)\n",
    "\n",
    "# Monthly disposable income\n",
    "log_df['mnthly_disposable_income'] = (log_df['annual_inc'] / 12) - log_df['installment']\n",
    "\n",
    "# Investor confidence\n",
    "log_df['investor_confidence'] = log_df['funded_amnt_inv_log'] / log_df['loan_amnt_log']\n",
    "\n",
    "# Utilization Ratios\n",
    "log_df['avg_utilization_ratio'] = (log_df['revol_util'] + log_df['bc_util']) / 2\n",
    "\n",
    "# Historical Delinquencies\n",
    "log_df['delinquency_score'] = log_df['acc_now_delinq'] + log_df['delinq_2yrs'] + log_df['num_accts_ever_120_pd']\n",
    "\n",
    "# Credit Inquiry Impact\n",
    "log_df['credit_inquiry_impact'] = (log_df['inq_last_12m'] * 0.5) + (log_df['inq_last_6mths'] * 0.75) + log_df['inq_fi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Creating Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['total_received'] = df['total_pymnt'] + df['recoveries'] - df['collection_recovery_fee']\n",
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:53: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ROI'] = ((df['total_received'] - df['funded_amnt_log']) / df['funded_amnt_log'].replace(0, np.nan)) * 100\n"
     ]
    }
   ],
   "source": [
    "# Calculate ROI\n",
    "log_df = utility.calculate_roi(df=log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:177: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ROI_descriptor'] = df['ROI'].apply(categorize_roi)\n"
     ]
    }
   ],
   "source": [
    "# Calculate ROI descriptor (High, Medium, Low) based on ROI\n",
    "log_df = utility.get_roi_descriptor(df=log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit classifier features list - can only include data that is available at the time the loan is drafted. Also removed any \n",
    "# features that were used to FE other ones to reduce dimensionality \n",
    "columns_to_drop = ['fico_range_high', 'fico_range_low', 'funded_amnt', 'funded_amnt_inv', 'il_util', 'int_rate', 'loan_amnt', 'verified_status_joint']\n",
    "classifier_features = [item for item in classifier_features if item not in columns_to_drop]\n",
    "classifier_features += ['ROI_descriptor']\n",
    "classifier_features += ['loan_amnt_log', 'funded_amnt_log', 'funded_amnt_inv_log', 'int_rate_log', 'last_fico_range_high_log', 'il_util_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['installment', 'annual_inc', 'funded_amnt_inv_log', 'loan_amnt_log',\n",
    "                   'revol_util', 'bc_util', 'acc_now_delinq', 'delinq_2yrs',\n",
    "                   'num_accts_ever_120_pd', 'inq_last_12m', 'inq_last_6mths', 'inq_fi', 'ROI'] + date_columns\n",
    "\n",
    "\n",
    "classifier_features = [item for item in classifier_features if item not in columns_to_drop]\n",
    "\n",
    "log_df = log_df[classifier_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].astype('category')\n",
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].astype('category')\n",
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].astype('category')\n",
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].astype('category')\n",
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].astype('category')\n",
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].astype('category')\n",
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].astype('category')\n",
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].astype('category')\n",
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].astype('category')\n",
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].astype('category')\n",
      "/nas/longleaf/home/aryonna/488-team-assignment-2/utils/utility.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].astype('category')\n"
     ]
    }
   ],
   "source": [
    "category_columns = utility.get_object_columns(df=log_df)\n",
    "log_df = utility.to_categorical(df=log_df, columns=category_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['addr_state', 'application_type', 'emp_length', 'grade', 'home_ownership', 'initial_list_status', 'purpose', 'sub_grade', 'term', 'verification_status']\n"
     ]
    }
   ],
   "source": [
    "# Run once - must remove ROI_descriptor because this is our target \n",
    "category_columns = utility.get_category_columns(df=log_df)\n",
    "category_columns.remove('ROI_descriptor')\n",
    "print(category_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding on all (except ROI_descriptor)\n",
    "log_df = utility.one_hot_encode(df=log_df, columns_to_encode=category_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save targets\n",
    "y = log_df['ROI_descriptor'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700923\n"
     ]
    }
   ],
   "source": [
    "# Check to see if number of y's = len of log_df\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = log_df.drop(columns='ROI_descriptor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1 Explain that X and y are Saved for Cross Validation and Good Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (DO ONCE: ALREADY DONE) Save so we can use it to cross validate later\n",
    "#dump(X, '/nas/longleaf/home/aryonna/488-team-assignment-2/data/raw_data/all_data/features.joblib')\n",
    "\n",
    "# Save labels\n",
    "#dump(y, '/nas/longleaf/home/aryonna/488-team-assignment-2/data/raw_data/all_data/target.joblib')\n",
    "\n",
    "# EXPLANATION: This code saves X and y to a .joblib file for a model to use later once trained\n",
    "# with a script. I'm just leaving this here so you can explain how/ where we got the data from. \n",
    "# This should not be ran. Simply here demo purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3 Explain that X_train and y_train are Saved for Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (DO ONCE: ALREADY DONE) SAVE FOR TRAINING THE MODEL\n",
    "#dump(X_train, '/nas/longleaf/home/aryonna/488-team-assignment-2/data/train_data/all_data/features.joblib')\n",
    "\n",
    "# Save labels\n",
    "#dump(y_train, '/nas/longleaf/home/aryonna/488-team-assignment-2/data/train_data/all_data/target.joblib')\n",
    "\n",
    "# EXPLANATION: This code saves X_train and y_train to a .joblib file for a model to use later to\n",
    "# train a model with a script. I'm just leaving this here so you can explain how/ where we got the data from. \n",
    "# This should not be ran. Simply here demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, these features were deemed the most important based on \"mean\" meaning their \n",
    "# imporatnce was over the average importance. These were generated from the rfc.py training script\n",
    "most_important_features = [\"dti\", \"max_bal_bc\", \"revol_bal\", \"tot_hi_cred_lim\", \"total_bal_ex_mort\", \n",
    "                           \"total_bal_il\", \"total_bc_limit\", \"funded_amnt_log\", \"int_rate_log\", \n",
    "                           \"last_fico_range_high_log\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4 Reduce Data Based on Fearture Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = X[most_important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE RAW REDUCED DATA FOR CROSS VALIDATION\n",
    "#dump(reduced_data, '/nas/longleaf/home/aryonna/488-team-assignment-2/data/raw_data/reduced_data/features.joblib')\n",
    "\n",
    "# Save labels\n",
    "#dump(y, '/nas/longleaf/home/aryonna/488-team-assignment-2/data/raw_data/reduced_data/target.joblib')\n",
    "\n",
    "# EXPLANATION: This code saves X and y to a .joblib file for cross validation on the important features\n",
    "# I'm just leaving this here so you can explain how/ where we got the data from. \n",
    "# This should not be ran. Simply here demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model that was generated in the rfc.py script\n",
    "rfc = load('/nas/longleaf/home/aryonna/488-team-assignment-2/models/rfc_reduced_data.joblib')\n",
    "y_pred = rfc.predict(X_test[most_important_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the accuracy on the test set\n",
    "accuracy_rf = rfc.score(X_test[most_important_features], y_test)\n",
    "print(f\"Accuracy of Random Forest: {accuracy_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path from which to read the results\n",
    "cross_val_results_path = '/nas/longleaf/home/aryonna/488-team-assignment-2/cross-val-results/rfc_reduced_results.txt'  # Update this to your actual file path\n",
    "\n",
    "# Open the file in read mode ('r') and print its contents\n",
    "with open(cross_val_results_path, 'r') as file:\n",
    "    results = file.read()\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Possible Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Conclusions and Findings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
