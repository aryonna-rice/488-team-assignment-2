{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analysis for Team Assignment 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Adjust the path below according to your project's structure\n",
    "# This should be the path to the directory containing 'notebooks' and 'utils'\n",
    "project_root = Path.cwd().parent  # If your notebook is directly inside the 'notebooks' directory\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.utility as utility\n",
    "import imputers.latest_credit_pull_d as lcpd\n",
    "import imputers.earliest_cr_line_d as ecld\n",
    "import imputers.last_pymnt_d as lpd\n",
    "import imputers.numeric_data as nd\n",
    "\n",
    "# We first import a number of libraries that we will be using in today's class\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting packages we'll use\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Rather than importing the whole sklearn library, we will import only certain modules\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Importing Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_rejected = '../data/LendingClub/rejected_2007_to_2018Q4.csv'\n",
    "path_to_accepted = '../data/LendingClub/accepted_2007_to_2018Q4.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Read rejected and accpeted data into two seperate dataframes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DFs\n",
    "df_rejected = pd.read_csv(path_to_rejected)\n",
    "df_accepted = pd.read_csv(path_to_accepted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rows and columns\n",
    "print(df_accepted.shape)\n",
    "print(df_rejected.shape)\n",
    "# 3. output first 5 observations\n",
    "#df_rejected.head()\n",
    "df_accepted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Examine features* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accepted.info(verbose=True, show_counts=True)\n",
    "df_rejected.info(verbose=True, show_counts=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Droping columns*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Dropping irrelevant columns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get positions of irrelevant columns\n",
    "rejected_positions_to_drop = [2, 8]\n",
    "accepted_positions_to_drop = [0, 1, 18, 19, 55, 21, 10, 22]\n",
    "\n",
    "# Get the column names based on their positions\n",
    "rejected_columns_to_drop = df_rejected.columns[rejected_positions_to_drop]\n",
    "accepted_columns_to_drop = df_accepted.columns[accepted_positions_to_drop]\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df_rejected = df_rejected.drop(rejected_columns_to_drop, axis=1)\n",
    "df_accepted = df_accepted.drop(accepted_columns_to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Dropping columns with a high null count*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop where the null count is greater than or equal to 50% of samples.\n",
    "df_accepted = utility.drop_null_columns(df_accepted)\n",
    "df_rejected = utility.drop_null_columns(df_rejected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rows and columns\n",
    "print(df_accepted.shape)\n",
    "print(df_rejected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df_accepted.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accepted = utility.remove_outliers(df=df_accepted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine data's distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some summary statistics\n",
    "df_rejected.describe()\n",
    "df_accepted.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the columns with high skew values as these need to be fixed later\n",
    "skewed_columns = utility.get_high_skewed_columns(df=df_accepted)\n",
    "print(skewed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize numerical data for accepted data\n",
    "utility.visualize_numerical_variables(df_accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize numerican data for rejected data\n",
    "utility.visualize_numerical_variables(df_rejected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Imputing missing values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all columns that are dates \n",
    "potential_date_columns = utility.get_category_columns(df_accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display each object with its value count\n",
    "value_counts = utility.display_value_counts(df_accepted, potential_date_columns)\n",
    "value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations from Value Counts\n",
    "*Upon looking at the value counts, the following observations were made:*\n",
    "\n",
    "- There are four dates: 'issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d'\n",
    "- These values must be properly dealt with to be used in our analysis\n",
    "- issue_d: \n",
    "    - Can not be imputed. Drop all rows where loan_d is null\n",
    "- earliest_cr_line:\n",
    "    - Should be imputed based on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows where issue_d is null\n",
    "df_accepted = df_accepted.dropna(subset=['issue_d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type cast all date columns to date objects\n",
    "date_columns = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d']\n",
    "df_accepted = utility.to_datetime(df=df_accepted, columns=date_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing dates\n",
    "ecld_imputer = ecld.EarliestCRLineDateImputer()\n",
    "ecld_imputer.fit(X=df_accepted)\n",
    "df_accepted = ecld_imputer.transform(X=df_accepted)\n",
    "\n",
    "lcpd_imputer = lcpd.LatestCreditPullDateImputer()\n",
    "lcpd_imputer.fit(X=df_accepted)\n",
    "df_accepted = lcpd_imputer.transform(X=df_accepted)\n",
    "\n",
    "lpd_imputer = lpd.LastPaymentDateImputer()\n",
    "lpd_imputer.fit(X=df_accepted)\n",
    "df_accepted = lpd_imputer.transform(X=df_accepted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop loan_status: Does not meet the credit policy. Status:Fully Paid, Does not meet the credit policy. Status:Charged Off,\n",
    "# emp_length: 10+ years, < 1 year\n",
    "df_accepted = df_accepted[(df_accepted.emp_length != '10+ years') &\n",
    "                          (df_accepted.emp_length != '< 1 year')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the rest categorical \n",
    "category_columns = utility.get_category_columns(df=df_accepted)\n",
    "result_list = [item for item in category_columns if item not in date_columns]\n",
    "df_accepted = utility.to_categorical(df=df_accepted, columns=result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accepted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing categorical data\n",
    "- Going to start off with SimpleImputer.\n",
    "- Change later to more complex and specific imputers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(category_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accepted.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute categorical data using mode\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df_accepted[category_columns] = imputer.fit_transform(df_accepted[category_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing numerical data\n",
    "*Lets breakdown each numerical column and make a decision on what imputation strategy would be best:*\n",
    "\n",
    "- `loan_amnt`: drop. Too important not to have and the number of rows without this amount is small.\n",
    "- `funded_amnt`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = nd.NumericDataImputer(group_column='loan_status')\n",
    "imputer.fit(df_accepted)\n",
    "df_accepted = imputer.transform(df_accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a copy of pre-engineered features\n",
    "pre_fe_df_accepted = df_accepted.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Dates Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineer credit history length and loan age\n",
    "collection_date = pd.to_datetime('2018-12-31')\n",
    "df_accepted['credit_history_length'] = (df_accepted['issue_d'] - df_accepted['earliest_cr_line']).dt.days / 365.25\n",
    "df_accepted['loan_age'] = (df_accepted['last_pymnt_d'] - df_accepted['issue_d']).dt.days / 30\n",
    "df_accepted['months_since_last_credit_pull'] = (collection_date - df_accepted['last_credit_pull_d']).dt.days / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop dates to reduce complexity\n",
    "df_accepted = utility.drop_nan(df_accepted, date_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accepted.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Examine target variable*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accepted = utility.calculate_roi(df=df_accepted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
